{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models\n",
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU Availability\n",
    "\n",
    "haveCuda = torch.cuda.is_available()\n",
    "#print(haveCuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# NN definition\n",
    "\n",
    "class CarBadnessGuesser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CarBadnessGuesser, self).__init__()\n",
    "\n",
    "        #self.dataset = read_data()\n",
    "        #self.valid_freq = 10\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=3, kernel_size=(10, 5, 5), stride=(10, 5, 5)),\n",
    "            nn.BatchNorm3d(3),\n",
    "            nn.Conv3d(in_channels=3, out_channels=2, kernel_size=5),\n",
    "            nn.BatchNorm3d(2),\n",
    "            nn.Conv3d(in_channels=2, out_channels=1, kernel_size=3),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.AdaptiveMaxPool3d((1, 1, 10)),\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=10, out_features=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=5, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        if haveCuda:\n",
    "            self.linear.cuda()\n",
    "            self.model.cuda()  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.model(x.unsqueeze(0).unsqueeze(0))\n",
    "        return self.linear(conv_out.squeeze(-2).squeeze(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Read data (returns datadict)\n",
    "\n",
    "def read_data(base_path='./Data',split_size=[0.4, 0.4, 0.2]):  \n",
    "    #  the split_size gives how much \n",
    "    #  of the data goes to the train/test.\n",
    "    \"\"\"\n",
    "    Reads all of the .mat files in the given base_path, and returns a dict with the data found there.\n",
    "    :param split_size:\n",
    "    :param base_path: The directory that should be read in.\n",
    "    :return: a dict, containing the EES and difference tensors.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for file in os.listdir(base_path):\n",
    "        i = i + 1\n",
    "    pbar = tqdm(total=i)\n",
    "\n",
    "    data_dict = {}\n",
    "    for file in os.listdir(base_path):\n",
    "        num, data_type = file.split('_')\n",
    "        data_type = data_type.split('.')[0]\n",
    "        num = int(num)\n",
    "        if \"EES\" in data_type:\n",
    "            tensor_in = sio.loadmat(os.path.join(base_path, file))['EES_value']\n",
    "            tensor_in = torch.FloatTensor(tensor_in).squeeze(0)\n",
    "        else:\n",
    "            tensor_in = sio.loadmat(os.path.join(base_path, file))['Kulonbseg']\n",
    "            tensor_in = torch.FloatTensor(tensor_in)\n",
    "        try:\n",
    "            data_dict[num][data_type] = tensor_in\n",
    "        except KeyError:\n",
    "            data_dict[num] = {data_type: tensor_in}\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "\n",
    "    new_data = []\n",
    "    for key in data_dict.keys():\n",
    "        new_data.append(data_dict[key])\n",
    "    if isinstance(split_size, list):\n",
    "        training_samples = int(split_size[0] * len(new_data))\n",
    "        valid_samples = int(split_size[1] * len(new_data))\n",
    "        test_samples = int(split_size[2] * len(new_data))\n",
    "        while sum([training_samples, valid_samples, test_samples]) != len(new_data):\n",
    "            training_samples += 1\n",
    "        new_datadict = {'train': new_data[:training_samples],\n",
    "                        'validation': new_data[training_samples:training_samples + valid_samples],\n",
    "                        'test': new_data[-test_samples:]}\n",
    "    else:\n",
    "        new_datadict = {'train': new_data,\n",
    "                        'validation': new_data,\n",
    "                        'test': new_data}\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return new_datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data = read_data()\n",
    "\n",
    "trainSet = data['train']\n",
    "testSet = data['test']\n",
    "# Dataloaders are responsible for giving random (if shuffle is true) minibatches\n",
    "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=5, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(testSet, batch_size=5, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss\n",
    "\n",
    "def createLoss():\n",
    "    return nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use cros entropy, since CIFAR10 is a classification set\n",
    "# def createLoss():\n",
    "#     return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Learning Rate Scheduler\n",
    "\n",
    "# Run for 50 epochs - 1 epoch means the networks sees every training image once\n",
    "numEpoch = 50\n",
    "\n",
    "# Cosine annealing learning rate scheduler - in 50 epochs the lr will become 0.01\n",
    "def createScheduler():\n",
    "    return optim.lr_scheduler.CosineAnnealingLR(optimizer,numEpoch,eta_min=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create Optimizer\n",
    "    \n",
    "def createOptimizer(self):\n",
    "    return optim.Adam(list(self.model.parameters()) + list(self.linear.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import optim\n",
    "\n",
    "# # Stochastic Gradient Descent (SGD) optimizer with Nesterov momentum and 0.1 learning rate\n",
    "# # Weight decay is the relative weight of the L2 regularization term\n",
    "# def createOptimizer():\n",
    "#     return optim.SGD(net.parameters(), lr=1e-1, momentum=0.9, nesterov=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "\n",
    "# Instantiate network and convert it to CUDA\n",
    "def createNet():\n",
    "    net = CarBadnessGuesser()\n",
    "    if haveCuda:\n",
    "        net = net.cuda()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# Function for training a single epoch\n",
    "def train(epoch):\n",
    "    # variables for loss\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "\n",
    "    # set the network to train (for batchnorm and dropout)\n",
    "    net.train()\n",
    "\n",
    "    # Create progress bar\n",
    "    bar = display(progress(0, len(trainLoader)), display_id=True)\n",
    "    \n",
    "    # data will contain one minibatch of images and correcponding labels\n",
    "    # When the iteration is finished we have seen every training image once\n",
    "    for i, data in enumerate(trainSet):\n",
    "        #breakpoint()\n",
    "        \n",
    "        input_data = data['KUL'].cuda()\n",
    "        prediction = net(input_data)\n",
    "        loss = criterion(prediction, data['EES'].cuda())\n",
    "        print('training- ', 'y^', prediction.item(), 'y', data['EES'].cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "        # Progress bar\n",
    "        bar.update(progress(i+1, len(trainLoader)))    \n",
    "    \n",
    "     # return loss and accuracy\n",
    "    tr_loss = running_loss / i\n",
    "    tr_corr = 0\n",
    "    #tr_corr = correct / total * 100\n",
    "    print(\"Train epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, tr_corr))\n",
    "    return tr_loss,tr_corr      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation\n",
    "\n",
    "# Function for validating a single epoch\n",
    "def val(epoch):\n",
    "    \n",
    "    # variables for loss\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    # set the network to eval  (for batchnorm and dropout)\n",
    "    net.eval()\n",
    "    \n",
    "    # Create progress bar\n",
    "    bar = display(progress(0, len(testLoader)), display_id=True)\n",
    "    \n",
    "    for i, data in enumerate(testSet):\n",
    "        input_data = data['KUL'].cuda()\n",
    "        prediction = net(input_data)\n",
    "        loss = criterion(prediction, data['EES'].cuda())\n",
    "        print('validation- ', 'y^', prediction.item(), 'y', data['EES'].cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "\n",
    "        bar.update(progress(i+1, len(testLoader)))\n",
    "\n",
    "    # return loss and accuracy\n",
    "    val_loss = running_loss / i\n",
    "    #val_corr = correct / total * 100\n",
    "    val_corr = 0\n",
    "    print(\"Test epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, val_corr))\n",
    "    return val_loss,val_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='20'\n",
       "            max='4',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            20\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-  y^ 0.33263492584228516 y tensor([11.4718], device='cuda:0')\n",
      "training-  y^ 0.33516037464141846 y tensor([0.5626], device='cuda:0')\n",
      "training-  y^ 0.3490506112575531 y tensor([0.0211], device='cuda:0')\n",
      "training-  y^ 0.32391422986984253 y tensor([53.0057], device='cuda:0')\n",
      "training-  y^ 0.3621421754360199 y tensor([16.1647], device='cuda:0')\n",
      "training-  y^ 0.37643787264823914 y tensor([0.5208], device='cuda:0')\n",
      "training-  y^ 0.4029316306114197 y tensor([3.6421], device='cuda:0')\n",
      "training-  y^ 0.39151668548583984 y tensor([0.2980], device='cuda:0')\n",
      "training-  y^ 0.4073716402053833 y tensor([40.6911], device='cuda:0')\n",
      "training-  y^ 0.40479081869125366 y tensor([1.3184], device='cuda:0')\n",
      "training-  y^ 0.4106837511062622 y tensor([11.8645], device='cuda:0')\n",
      "training-  y^ 0.4151933491230011 y tensor([2.9321], device='cuda:0')\n",
      "training-  y^ 0.35715633630752563 y tensor([54.6192], device='cuda:0')\n",
      "training-  y^ 0.42532670497894287 y tensor([0.0554], device='cuda:0')\n",
      "training-  y^ 0.4303973615169525 y tensor([0.1216], device='cuda:0')\n",
      "training-  y^ 0.4353049397468567 y tensor([0.0672], device='cuda:0')\n",
      "training-  y^ 0.3984602391719818 y tensor([18.9842], device='cuda:0')\n",
      "training-  y^ 0.42198076844215393 y tensor([40.3548], device='cuda:0')\n",
      "training-  y^ 0.4458054006099701 y tensor([0.5009], device='cuda:0')\n",
      "training-  y^ 0.4365726113319397 y tensor([10.6966], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tr_corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1622713fdafd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtrain_accs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-0725db2c02d6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#tr_corr = correct / total * 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train epoch %d loss: %.3f correct: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_corr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_corr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tr_corr' is not defined"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "#  Containers for losses and accuracies for every epoch\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "# Best validation accuracy\n",
    "best_acc = 0\n",
    "\n",
    "# Set pseudo-random generator seeds to make multiple runs comparable\n",
    "torch.manual_seed(1)\n",
    "if haveCuda:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# Create net, criterion, optimizer and scheduler\n",
    "# This needs to be done after setting the random seed, \n",
    "# so that the random initialization would be the same\n",
    "net = createNet()\n",
    "criterion = createLoss()\n",
    "optimizer = createOptimizer(net)\n",
    "# scheduler = createScheduler()\n",
    "\n",
    "# For numEpoch epochs\n",
    "for epoch in range(numEpoch):\n",
    "    \n",
    "    # The with the LR scheduler\n",
    "#     scheduler.step()\n",
    "    \n",
    "    # Train\n",
    "    loss,acc = train(epoch)\n",
    "    train_accs.append(acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # Validate\n",
    "    loss,acc = val(epoch)\n",
    "    val_accs.append(acc)\n",
    "    val_losses.append(loss)\n",
    "    \n",
    "    # If the current model is better, than the previous best, save it\n",
    "    if acc > best_acc:\n",
    "        print(\"Best Model, Saving\")\n",
    "        best_acc = acc\n",
    "        torch.save(net,\"./data/model.pth\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #Validation\n",
    "    \n",
    "# def validation(self):\n",
    "#     \"\"\"\n",
    "#     Runs the validation phase of the training\n",
    "#     :return: The validation loss average\n",
    "#     \"\"\"\n",
    "#     self.eval()\n",
    "#     average_loss = 0\n",
    "#     step = 0\n",
    "#     for step, data in enumerate(self.dataset['validation']):\n",
    "#         with torch.no_grad():\n",
    "#             input_data = data['KUL'].cuda()\n",
    "#             prediction = self(input_data)\n",
    "#             loss = self.loss_fn(prediction, data['EES'].cuda())\n",
    "#             average_loss += loss.item()\n",
    "#     print(\"Validation is complete\")\n",
    "#     return average_loss / (step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "\n",
    "# def train(self, epochs=100):\n",
    "#     b_loss = []\n",
    "#     v_loss = []\n",
    "#     for epoch in trange(epochs):\n",
    "#         for step, data in enumerate(self.dataset[\"train\"]):\n",
    "#             input_data = data['KUL'].cuda()\n",
    "#             prediction = self(input_data)\n",
    "#             loss = self.loss_fn(prediction, data['EES'].cuda())\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "#             self.zero_grad()\n",
    "#             b_loss.append(loss.item())\n",
    "\n",
    "#         if epoch % self.valid_freq and epoch != 0:\n",
    "#             print(f'Validation loss: {self.validation()}', flush=True)\n",
    "#             v_loss.append(self.validation())\n",
    "\n",
    "#     print(\"Train is complete\")\n",
    "\n",
    "#     #self.test()\n",
    "#     #self.save_model()\n",
    "#     plt.plot(v_loss)\n",
    "#     plt.ylabel('Validation loss')\n",
    "#     plt.show()\n",
    "#     plt.plot(b_loss)\n",
    "#     plt.ylabel('Batch loss')\n",
    "#     plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "        \"\"\"\n",
    "        Runs the evaluation of the network.\n",
    "        :return: average loss for the test\n",
    "        \"\"\"\n",
    "        average_loss = 0\n",
    "        step = 0\n",
    "        for step, data in enumerate(self.dataset['test']):\n",
    "            with torch.no_grad():\n",
    "                input_data = data['KUL'].cuda()\n",
    "                prediction = self(input_data)\n",
    "                loss = self.loss_fn(prediction, data['EES'].cuda())\n",
    "                average_loss += loss.item()\n",
    "        print(\"the test is complete\")\n",
    "        return average_loss / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(self, save_dir=\"./training\"):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S.%d\")\n",
    "        save_path = os.path.join(save_dir, timestamp)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, os.path.join(save_path, 'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    Saves weights to the given directory plus the timestamp\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    save_dir=\"./training\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S.%d\")\n",
    "    save_path = os.path.join(save_dir, timestamp)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.pth'))\n",
    "    print(\"saving weights is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "# Containers for losses and accuracies for every epoch\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "# Best validation accuracy\n",
    "best_acc = 0\n",
    "\n",
    "# Set pseudo-random generator seeds to make multiple runs comparable\n",
    "torch.manual_seed(1)\n",
    "if haveCuda:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# Create net, criterion, optimizer and scheduler\n",
    "# This needs to be done after setting the random seed, \n",
    "# so that the random initialization would be the same\n",
    "net = createNet()\n",
    "criterion = createLoss()\n",
    "# optimizer = createOptimizer()\n",
    "# scheduler = createScheduler()\n",
    "\n",
    "# # For numEpoch epochs\n",
    "# for epoch in range(numEpoch):\n",
    "    \n",
    "#     # The with the LR scheduler\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     # Train\n",
    "#     loss,acc = train(epoch)\n",
    "#     train_accs.append(acc)\n",
    "#     train_losses.append(loss)\n",
    "    \n",
    "#     # Validate\n",
    "#     loss,acc = val(epoch)\n",
    "#     val_accs.append(acc)\n",
    "#     val_losses.append(loss)\n",
    "    \n",
    "#     # If the current model is better, than the previous best, save it\n",
    "#     if acc > best_acc:\n",
    "#         print(\"Best Model, Saving\")\n",
    "#         best_acc = acc\n",
    "#         torch.save(net,\"./data/model.pth\")\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    learner = CarBadnessGuesser()\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    learner.train()\n",
    "    save_model(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image, model):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='./Data'\n",
    "for file in os.listdir(base_path):\n",
    "        data_dict = {}\n",
    "        num, data_type = file.split('_')\n",
    "        data_type = data_type.split('.')[0]\n",
    "        num = int(num)\n",
    "        if \"EES\" in data_type:\n",
    "            tensor_in = sio.loadmat(os.path.join(base_path, file))['EES_value']\n",
    "            tensor_in = torch.FloatTensor(tensor_in).squeeze(0)\n",
    "        else:\n",
    "            tensor_in = sio.loadmat(os.path.join(base_path, file))['Kulonbseg']\n",
    "            tensor_in = torch.FloatTensor(tensor_in)\n",
    "        try:\n",
    "            data_dict[num][data_type] = tensor_in\n",
    "        except KeyError:\n",
    "            data_dict[num] = {data_type: tensor_in}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    new_data = []\n",
    "    for key in data_dict.keys():\n",
    "        new_data.append(data_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = new_data['KUL']\n",
    "prediction = self(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
